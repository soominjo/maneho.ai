# Technical Design Document (TDD): Maneho.ai

**Version:** 1.0
**Project:** Maneho.ai (LTO RAG Assistant)

**Architecture:** Hytel Monorepo (pnpm / Turborepo)
**Infrastructure:** Firebase (Blaze) + Google Cloud Platform (Vertex AI)

## 1. System Architecture Overview

Maneho.ai utilizes a production-ready monorepo template built with pnpm and Turborepo for optimal developer experience. The system relies on a backend-heavy architecture to protect API keys, enforce daily rate limits, and safely orchestrate the Retrieval-Augmented Generation (RAG) pipeline.

### Monorepo Workspaces

- **`apps/web`**: The React frontend using Vite and Tailwind CSS. It consumes the backend API via TanStack Query and tRPC.

- **`apps/functions`**: The tRPC backend running on Firebase Cloud Functions (Node 20 / TypeScript). This houses the RAG logic, Gemini API calls, and Vector Search operations.

- **`packages/ui`**: Shared React components, specifically Shadcn UI components (e.g., Button, Card) and Tailwind utilities.

- **`packages/shared`**: Shared Zod schemas ensuring type-safe validation everywhere, bridging the frontend and backend.

## 2. RAG Infrastructure (Firebase + Vertex AI)

To eliminate third-party vector databases and rely solely on the GCP ecosystem, the application uses Firestore and Vertex AI Vector Search.

- **Canonical Storage (`Firestore`)**: Stores the original LTO document metadata, chunked text, source URIs, and tags.
- **Vector Storage (`Vertex AI Vector Search`)**: Stores only the embeddings generated by `gemini-embedding-001` (3072 dimensions) and their corresponding Firestore datapoint IDs. We will maintain one index and one public endpoint to strictly control the "always-on" VM costs.
- **Orchestration (`Google Gen AI SDK`)**: The `apps/functions` environment uses `@google/genai` to handle embeddings and generate grounded responses using `gemini-2.5-flash`.

## 3. Data Flow & Core Workflows

### A. Document Ingestion Pipeline (Admin Only)

1. Admin uploads an LTO PDF to **Firebase Storage**.
2. An event-triggered Cloud Function (`apps/functions`) detects the file.
3. The function extracts the text, chunks it (e.g., overlapping 1000-character blocks), and embeds it using `gemini-embedding-001`.
4. The chunks and metadata are saved to **Firestore**.
5. The vectors and mapping IDs are upserted to the **Vertex AI Vector Search** REST endpoint.

### B. "The Lawyer" Chat Workflow (User Query)

1. User submits a question (e.g., "What is the fine for swerving?") via `apps/web`.
2. The frontend calls a tRPC procedure (e.g., `trpc.chat.askLawyer.useMutation()`).

3. The backend validates the input against a Zod schema defined in `packages/shared`.

4. The backend verifies the user's daily quota (max 20 queries).
5. The backend embeds the user's query and fetches the top 6 nearest neighbors from **Vertex AI Vector Search**.
6. The backend retrieves the actual text chunks from **Firestore** using the returned IDs.
7. The backend prompts `gemini-2.5-flash` with the system prompt, the retrieved LTO context chunks, and the user's query.
8. The grounded response and citations are returned to the frontend.

### C. "Ticket Decoder" Workflow (Vision API)

1. User uploads an image of an LTO ticket.
2. The frontend uploads the image to **Firebase Storage**.
3. A tRPC procedure passes the image URL to `gemini-2.5-flash` (multimodal).
4. The AI extracts the handwritten violation code/text.
5. The backend automatically pipes this extracted text into the standard RAG workflow (Section 3B) to retrieve the official fines.

## 4. API Design (tRPC)

tRPC provides end-to-end type safety between the frontend and backend.

**Key Routers (`apps/functions/src/trpc/`)**:

- `chatRouter`: Handles the RAG queries, prompt script generation, and ticket decoding logic.
- `userRouter`: Handles quota tracking and profile management.
- `adminRouter`: Handles manual ingestion triggers and index management.

## 5. CI/CD & Security

- **Workload Identity Federation (WIF)**: All deployments use keyless authentication with GCP, meaning there are no stored service account keys and it uses short-lived tokens.

- **GitHub Actions**:
- `ci.yml`: Triggered on PR and push to lint, typecheck, build, and test.

- `deploy-main.yml`: Manual deployment to production using WIF.

- **Pre-commit Hooks**: Husky automatically runs ESLint and Prettier on staged files before committing.

## 6. System Requirements

To develop and deploy this application, the following minimum versions are strictly required:

- Node.js 20.x

- pnpm 8.x

- Turbo 2.x

- TypeScript 5.x

- Firebase CLI 13.x (for deployment)

---

Would you like me to map out the exact Zod schemas for `packages/shared` that we'll need for the Chat and Ingestion API inputs next?
